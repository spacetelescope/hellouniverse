{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d30c40a",
   "metadata": {},
   "source": [
    "# Estimating TESS rotation periods with CNNs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb9b70ac",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The NASA TESS mission conducts an all-sky survey searching for exoplanets that transit their host stars. To do so, it collects time series photometry called \"light curves\" for millions of stars across the sky. These light curves have many science uses besides exoplanets, including stellar rotation. As a star rotates, cool magnetic spots come into and out of view, causing periodic wiggles in the brighness measurements through time. We can therefore use light curves from TESS to infer stellar rotation periods, which are useful for studying stellar magnetism, structure, and ages. \n",
    "\n",
    "However, systematics associated with the telescope's special Earth-Moon orbit make it difficult to measure long (> 13 day) rotation periods from TESS light curves using conventional frequency analysis techniques. Machine learning methods, and in particular Convolutional Neural Networks (CNN) have been shown to circumvent some of the effects of systematics and estimate rotation periods from TESS light curves and their frequency transforms.\n",
    "\n",
    "In this tutorial we will use a CNN to estimate stellar rotation periods from frequency transforms of TESS light curves. For our training set, we will use the simulations from the MAST High Level Science Product [SMARTS](https://archive.stsci.edu/hlsp/smarts). SMARTS combines physically realistic simulations of rotational light curves with real noise and systematics from TESS. This combination allows CNNs to learn the difference between rotation signals and systematics and estimate stellar rotation periods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1b3c64b",
   "metadata": {},
   "source": [
    "## Goals\n",
    "The goal of this notebook is to use [SMARTS](https://archive.stsci.edu/hlsp/smarts) data to train a CNN to regress TESS rotation periods. We will\n",
    "\n",
    "0. [Configure the training run](#0.configure-training-run),\n",
    "1. [Prepare the training data](#1.prepare-training-data),\n",
    "2. [Build the CNN](#2.build-the-cnn),\n",
    "3. [Define training, validation, and evaluation functions](#3.define-training-validation-and-evaluation-functions),\n",
    "4. [Train the CNN](#4.train-the-cnn-on-smarts-data), and\n",
    "5. [Test the CNN performance](#5.test-the-cnn-performance).\n",
    "\n",
    "The training examples are 2-dimensional arrays of wavelet transforms of TESS light curves. The wavelet transform concentrates the periodicity of the light curve, making it easier for a CNN to regress the period. CNNs take advantage of the image-like nature of a wavelet transform in the same way that CNNs are useful for image recognition and computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe1acac-b293-4eeb-81d3-ff5553b1fadc",
   "metadata": {},
   "source": [
    "## Runtime\n",
    "\n",
    "On the [TIKE](https://timeseries.science.stsci.edu) \"Large\" instance, this notebook takes just under 3 minutes to run from start to finish. The bulk of this time is spent downloading the training data. Once you've done that once, you can comment out the cell, and the notebook will be faster.\n",
    "\n",
    "This notebook, including the CNN training, is configured to run on a single CPU."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "591c9b26",
   "metadata": {},
   "source": [
    "## Installs and Imports\n",
    "This notebook uses the following packages:\n",
    "- `glob` for generating lists of training files\n",
    "- `copy` for saving training weights\n",
    "- `numpy` for array operations\n",
    "- `matplotlib` for plotting\n",
    "- `astropy` for reading FITS files\n",
    "- `torch` for tensor and CNN operations\n",
    "\n",
    "You can install the requirements by running `%pip install -r requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71cc70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob  # for generating lists of input files\n",
    "from copy import deepcopy  # for saving CNN weights\n",
    "\n",
    "import numpy as np  # array operations\n",
    "from astropy.io import fits  # FITS file operations\n",
    "import matplotlib.pyplot as plt  # for plotting\n",
    "\n",
    "# For tensor and CNN operations\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2412b0aa-1cb5-46db-bea1-074821fb37fd",
   "metadata": {},
   "source": [
    "## 0. Configure training run\n",
    "\n",
    "Before we get started, we need to set some configuration parameters to know how much data to read in, and how many epochs to train for. Ideally, we want to run on the full training set (`max_n = 1_000_000`) for long enough that the loss plateaus (usually `num_epochs = 500`). But this for this simple demo, we will use a subset of training data and a shorter training time. We will train/validation/test using a 80%/10%/10% split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40253f60-adaa-49cc-aaab-ad708b4d71f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n = 10_000  # max 1_000_000\n",
    "num_epochs = 20  # 100 is pretty good, typically ~300 to 500 to reach a plateau\n",
    "split = [8, 1, 1]  # ratios for train/validation/test split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "766eb96b",
   "metadata": {},
   "source": [
    "## 1. Prepare training data\n",
    "\n",
    "The SMARTS training data are in the form of continuous Morlet wavelet transforms (CWTs) or wavelet power spectra (WPS). Rather than train on the light curve itself, the WPS provides a 2D representation of the frequency information present in the light curve. With frequency or period on the y-axis and time on the x-axis, the WPS illustrates which frequencies dominate the light curve at different times. Since it is effectively an image, we can take advantage of image recognition capabilities of CNNs.\n",
    "\n",
    "For more information on CWTs, see\n",
    "- [_A Practical Guide to Wavelet Analysis_, Torrence & Compo (1998)](https://ui.adsabs.harvard.edu/abs/1998BAMS...79...61T/abstract)\n",
    "- [The `pycwt` Python package](https://github.com/regeirk/pycwt)\n",
    "\n",
    "We will first download and extract the data, load it into a DataLoader, and then look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7388dbd-7e1e-4c4b-b660-781801dbadba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract training data\n",
    "# Comment this cell out if you've already downloaded and extracted the data once!\n",
    "!curl -o hlsp_smarts_tess_ffi_all_tess_v1.0_cat.tar.gz https://archive.stsci.edu/hlsps/smarts/tess/hlsp_smarts_tess_ffi_all_tess_v1.0_cat.tar.gz\n",
    "!tar -xvzf hlsp_smarts_tess_ffi_all_tess_v1.0_cat.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52414b4-5826-4db2-9fed-37784fd62b7f",
   "metadata": {},
   "source": [
    "We will partition the data into three sets: a training, validation, and test set.\n",
    "- The training set is used to fit the CNN parameters\n",
    "- The validation set is held-out for use to determine when to stop training. The training loss will decrease indefinitely, but the validation loss will stop decreasing when the CNN begins to overfit, signalling that it's reached a local maximum in its ability to generalize to new data.\n",
    "- The test set is used for the final performance evaluation.\n",
    "\n",
    "We use Pytorch `Dataset` (subclassed here) and `DataLoader` (used below) to load the wavelet data. Pytorch `Dataset` and `DataLoader` can be accessed in batches, which makes training more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a002abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset and Dataloader classes\n",
    "class WaveletDataset(Dataset):\n",
    "    \"\"\"\n",
    "    WaveletDataset to read in the training data.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    periods: array of rotation period corresponding to each wavelet transform.\n",
    "\n",
    "    wavelets: array containing the stack of wavelet transforms.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, periods, wavelets, mode, random_seed=42, max_n=10000, split=(8, 1, 1), normalize=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        `periods` (numpy.ndarray): the array of rotation periods, or labels.\n",
    "\n",
    "        `wavelets` (numpy.ndarray): the array of wavelet power spectra, or features.\n",
    "\n",
    "        `mode` (str): must be one of \"train\", \"validation\", or \"test\". Loads different data\n",
    "            depending on the specified mode.\n",
    "\n",
    "        `random_seed` (int, optional): seed for random number generator for \n",
    "            reproducibility.\n",
    "\n",
    "        `max_n` (int, optional): the number of training examples to use.\n",
    "\n",
    "        `split` (list-like): the split fractions for train/validation/test partitions\n",
    "\n",
    "        `normalize` (bool): whether to divide the periods and wavelets by their maximum values.\n",
    "        \"\"\"\n",
    "\n",
    "        # create shuffled index and shuffle arrays\n",
    "        np.random.seed(random_seed)\n",
    "        idx = np.random.choice(np.arange(len(periods), dtype=int), size=max_n, replace=False)\n",
    "        p = periods[idx]\n",
    "        w = wavelets[idx]\n",
    "\n",
    "        if normalize:\n",
    "            pmax = periods.max().item()\n",
    "            wmax = wavelets.max().item()\n",
    "            p = (p/pmax).astype(np.float32)\n",
    "            w = (w/wmax).astype(np.float32)\n",
    "        else:\n",
    "            pmax = np.nan\n",
    "            wmax = np.nan\n",
    "            p = p.astype(np.float32)\n",
    "            w = w.astype(np.uint8)\n",
    "        self.pmax = pmax\n",
    "        self.wmax = wmax\n",
    "\n",
    "        # determine how many examples to use for each partition\n",
    "        n_train, n_val, _ = (max_n * np.array(split)/np.sum(split)).astype(int).cumsum()\n",
    "\n",
    "        if mode == \"train\":\n",
    "            p = p[:n_train]\n",
    "            w = w[:n_train]\n",
    "        elif mode == \"validation\":\n",
    "            p = p[n_train:n_val]\n",
    "            w = w[n_train:n_val]\n",
    "        elif mode == \"test\":\n",
    "            p = p[n_val:]\n",
    "            w = w[n_val:]\n",
    "        else:\n",
    "            raise ValueError(\"`mode` must be one of 'train', 'validation', or 'test'.\")\n",
    "\n",
    "        # Assign periods and wavelets to class attributes\n",
    "        self.wavelets = w\n",
    "        self.periods = p\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of training examples in the Dataset.\n",
    "        \"\"\"\n",
    "        return len(self.periods)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        The data accessor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        `idx` (list-like): the list of indices to be accessed\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        `X` (tensor): substack of wavelet transforms\n",
    "\n",
    "        `label` (tensor): sub-array of rotation periods\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        X = torch.tensor(self.wavelets[idx], dtype=torch.float32).unsqueeze(0)\n",
    "        label = torch.tensor(self.periods[idx, np.newaxis])\n",
    "        return X, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754d5099",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read SMARTS data into data loaders\n",
    "filename = \"hlsp_smarts_tess_ffi_all_tess_v1.0_sim.fits\"\n",
    "print(f\"Reading data from {filename}...\", end=\"\")\n",
    "\n",
    "with fits.open(filename, memmap=True) as f:\n",
    "    p = f[1].data[\"Period\"]\n",
    "    w = f[2].data\n",
    "    train_dataset = WaveletDataset(p, w, mode=\"train\", max_n=max_n, split=split)\n",
    "    valid_dataset = WaveletDataset(p, w, mode=\"validation\", max_n=max_n, split=split)\n",
    "    test_dataset = WaveletDataset(p, w, mode=\"test\", max_n=max_n, split=split)\n",
    "\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Storing training data into DataLoaders...\", end=\"\")\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=50)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=50)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01d6ea4-db7b-4aa9-b77c-218b78f8fe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an example WPS\n",
    "wl, p = train_dataset[8]\n",
    "\n",
    "wldata = wl.squeeze().numpy()\n",
    "\n",
    "plt.figure()\n",
    "plt.pcolormesh(\n",
    "    np.linspace(0, 360, len(wldata)),  # time baseline is about a year\n",
    "    np.geomspace(0.1, 180, len(wldata)),  # period axis goes from 0.1 to 180\n",
    "    wldata,\n",
    "    shading=\"nearest\",\n",
    "    cmap=\"binary_r\"\n",
    ")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Time (days)\")\n",
    "plt.ylabel(\"Period (days)\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"SMARTS Wavelet Power Spectrum\")\n",
    "plt.colorbar(label=\"Normalized Power\")\n",
    "\n",
    "# Plot the actual rotation period of the simulated star\n",
    "plt.axhline(180*p.numpy().item(), color=\"r\", linestyle=\":\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71a068b-7f66-48fd-b8fd-c4ed4120cca4",
   "metadata": {},
   "source": [
    "This is an example of a WPS. For sinusoidal signals, there is a horizontal band of power at the dominant frequency. This example star has an equatorial rotation period of about 9 days, but the dominant frequency from the power spectrum is about 10.5 days. This is due to surface differential rotation, where spots emerge at higher latitudes that rotate more slowly than the equator.\n",
    "\n",
    "This panel (without the axes or colorbar) is what the CNN will be trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0173b8-c644-44a8-bfc6-139ffcc5afaa",
   "metadata": {},
   "source": [
    "## 2. Build the CNN\n",
    "\n",
    "We will build a CNN that takes 2D input (the WPS) and predicts two values: the stellar rotation period and its corresponding uncertainty. The CNN has the following aspects:\n",
    "- Three 2D convolution layers for feature extraction\n",
    "- ReLU activation\n",
    "- 1D max pooling in the time axis, for dimensionality reduction without losing frequency resolution.\n",
    "- Dropout to build redundancy and avoid overfitting\n",
    "- Softplus output for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55069762-c646-4d80-9313-1b0f23be16bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A relatively simple 2D Convolutional Neural Network with a configurable\n",
    "    number of trainable convolution kernels.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    c (list of ints, [8, 16, 32]): List of convolutional kernel depths.\n",
    "    \n",
    "    k (int or list of ints, 3): Convolutional kernel widths. If an int is\n",
    "        passed, it will be multiplied into a list of length `len(c)`.\n",
    "    \"\"\"\n",
    "    def __init__(self, c=[8, 16, 32], k=3):\n",
    "        if isinstance(k, int):\n",
    "            k = [k]*len(c)\n",
    "            \n",
    "        n_nodes = (64 - (sum(k) - len(k))) * c[-1]\n",
    "\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(  1,  c[0], k[0], 1)  # 62 x 20\n",
    "        self.conv2 = nn.Conv2d(c[0], c[1], k[1], 1)  # 60 x 6 \n",
    "        self.conv3 = nn.Conv2d(c[1], c[2], k[2], 1)  # 58 x 1\n",
    "        self.fc1 = nn.Linear(n_nodes, 256)  # 58 x 32 = 1856\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout2d(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, (1, 3))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, (1, 3))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, (1, x.shape[-1]))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout1(x)\n",
    "        output = F.softplus(self.fc3(x))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5c0504-7b03-4984-bae1-0dca8119c668",
   "metadata": {},
   "source": [
    "Here we will also define our loss function. A typical loss function for regression is the mean squared error (MSE). However, MSE doesn't allow for the prediction of uncertainties. Alternative loss functions that allow the prediction of uncertainties include the Laplacian and Gaussian Negative Log-Likelihoods (NLL). Gaussian NLL is analogous to MSE, while Laplacian NLL is analogous to median absolute error, which is less biased by outliers and more accurately predicts values near the edges of the training distribution. For these reasons, we will use the Laplacian NLL, which has the form\n",
    "\n",
    "$L = \\frac{1}{2b} \\exp(\\frac{-|x - \\mu|}{b})$,\n",
    "\n",
    "where $\\mu$ is the mean, and $b$ is related to the variance $\\sigma^2$ by $\\sigma^2 = 2 b^2$.\n",
    "\n",
    "The negative log-likelihood is then\n",
    "\n",
    "$-\\log L = \\frac{|x - \\mu|}{b} + \\ln(2b)$.\n",
    "\n",
    "You may find that other loss functions work better for your use case. We recommend trying several."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cf8b3f-1fb6-44f2-b07b-d7a5596eec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacian_nll(y_pred, y_true, k=1):\n",
    "    \"\"\"\n",
    "    Compute Negative Log Likelihood for Laplacian Output Layer. This loss function\n",
    "    lets the CNN predict a value with a related uncertainty.\n",
    "\n",
    "    Args:\n",
    "        y_pred: Nx2k matrix of parameters. Each row parametrizes\n",
    "                k Laplacian distributions, each with (mean, std).\n",
    "        y_true: Nxk matrix of (data) target values.\n",
    "    \"\"\"\n",
    "    means = y_pred[:, :k]\n",
    "    sigmas = y_pred[:, k:]\n",
    "\n",
    "    # convert from sigma to b\n",
    "    b = sigmas / np.sqrt(2) \n",
    "\n",
    "    # compute NLL\n",
    "    nll = torch.abs(means - y_true)/b + torch.log(2*b)\n",
    "    return nll\n",
    "\n",
    "def gaussian_nll(y_pred, y_true, k=1):\n",
    "    \"\"\"\n",
    "    Compute Negative Log Likelihood for Gaussian Output Layer. This loss function\n",
    "    lets the CNN predict a value with a related uncertainty.\n",
    "\n",
    "    Args:\n",
    "        y_pred: Nx2k matrix of parameters. Each row parametrizes\n",
    "                k Laplacian distributions, each with (mean, std).\n",
    "        y_true: Nxk matrix of (data) target values.\n",
    "    \"\"\"\n",
    "    means = y_pred[:, :k]\n",
    "    sigmas = y_pred[:, k:] \n",
    "\n",
    "    # compute NLL\n",
    "    nll = ((means - y_true)/sigmas)**2 + torch.log(2*np.pi*sigmas)\n",
    "    return nll/2\n",
    "\n",
    "# Set the chosen loss function here\n",
    "loss_function = laplacian_nll"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48006ccb",
   "metadata": {},
   "source": [
    "## 3. Define training, validation, and evaluation functions\n",
    "\n",
    "We train the CNN using the Adam optimizer, which uses adaptive learning rates (LR) to train the network. To vary the LR, we use a plateau scheduler (`ReduceLROnPlateau`), which reduces the LR when the loss plateaus. This enables the CNN parameters to find local minima more easily, rather than take large steps over them.\n",
    "\n",
    "Finally, we will also use an early stopping criterion. This means that if the validation loss plateaus or increases for a certain number of epochs, training stops early, and the best fit CNN values are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034d9728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, num_epochs=100, early_stopping_patience=10, device=torch.device(\"cpu\")):\n",
    "    \"\"\"Train the neural network for all desired epochs.\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
    "    # Set learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.7, patience=3)\n",
    "\n",
    "    # Set up training loop\n",
    "    train_p_loss = []\n",
    "    val_p_loss = []\n",
    "    min_loss = 100\n",
    "    early_stopping_count = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Compute a single epoch of training\n",
    "        p_loss = train_epoch(model, train_loader, optimizer, epoch, device=device)\n",
    "        train_p_loss.append(p_loss)\n",
    "        p_loss = evaluate(model, valid_loader)\n",
    "        val_p_loss.append(p_loss)\n",
    "        total_loss = p_loss\n",
    "        scheduler.step(total_loss)  # step learning rate scheduler\n",
    "\n",
    "        # if new fit is better than the previous best fit, update best fit weights\n",
    "        if total_loss < min_loss:\n",
    "            min_loss = total_loss\n",
    "            early_stopping_count = 0\n",
    "            best_epoch = epoch\n",
    "            best_weights = deepcopy(model.state_dict())\n",
    "        # otherwise, if loss is not getting better, count down to stopping criterion\n",
    "        else:\n",
    "            early_stopping_count += 1\n",
    "            print(f\"Early Stopping Count: {early_stopping_count}\")\n",
    "            if early_stopping_count == early_stopping_patience:\n",
    "                print(f\"Early Stopping. Best Epoch: {best_epoch} with loss {min_loss:.4f}.\")\n",
    "                with open(\"best_epoch.txt\", \"w\") as f:\n",
    "                    print(best_epoch, file=f)\n",
    "                break    \n",
    "\n",
    "    # save and return the best fit weights\n",
    "    torch.save(best_weights, f\"model.pt\")\n",
    "    return best_weights, train_p_loss, val_p_loss\n",
    "\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, epoch, device=torch.device(\"cpu\")):\n",
    "    \"\"\"Train the network for a single epoch.\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    period_losses = []\n",
    "    # iterate over data batches\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device, dtype=torch.float), target.to(device, dtype=torch.float)\n",
    "        optimizer.zero_grad()  # Clear the gradient\n",
    "        output = model(data)  # Make predictions\n",
    "        loss = loss_function(output, target).mean()\n",
    "        loss.backward()  # Gradient computation        \n",
    "        optimizer.step()  # Perform a single optimization step\n",
    "        period_losses.append(loss.item())\n",
    "\n",
    "        # print progress every few batches\n",
    "        if (batch_idx*len(data)) % 500 == 0:\n",
    "            print(\"Epoch: {:3d} [{:5d}/{:5d} ({:3.0f}%)] Training Loss: {:9.6f}\".format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), period_losses[-1]))\n",
    "            \n",
    "    # return the mean loss value\n",
    "    return np.mean(period_losses)\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, verbose=True, device=torch.device(\"cpu\"), return_predictions=False):\n",
    "    \"\"\"\n",
    "    Evaluate network on validation or test data and compute loss.\n",
    "    This is like \"train\" except the weights are not updated.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to inference mode\n",
    "    test_p_loss = 0\n",
    "    targets = []\n",
    "    preds = []\n",
    "    with torch.no_grad():  # For the inference step, gradient is not computed\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device, dtype=torch.float), target.to(device, dtype=torch.float)\n",
    "            output = model(data)\n",
    "            targets.extend(target.cpu().numpy())\n",
    "            preds.extend(output.cpu().numpy())\n",
    "            test_p_loss += loss_function(output, target).sum()\n",
    "            \n",
    "    test_p_loss /= len(test_loader.dataset)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Test loss: {test_p_loss:.4f}\")\n",
    "    if return_predictions:\n",
    "        return test_p_loss, np.squeeze(preds), np.squeeze(targets)\n",
    "    return test_p_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a59eb327",
   "metadata": {},
   "source": [
    "## 4. Train the CNN on SMARTS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c27331-b0e0-4286-8de1-d98693ea357b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize CNN\n",
    "model = ConvNet()\n",
    "\n",
    "# Train CNN\n",
    "weights, train_p_loss, val_p_loss = train(model, train_loader, valid_loader,\n",
    "    early_stopping_patience=10, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbed6fee-f76a-4bc8-a60e-40719a00287a",
   "metadata": {},
   "source": [
    "Now that the CNN is trained, we should take a look at the \"learning curves,\" or the loss over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9529bea-c8f8-41cb-86d3-d161d6f9b0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_p_loss, label=\"Training Loss\")\n",
    "plt.plot(val_p_loss, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aceb260-aa09-48b5-93fc-16d71665f042",
   "metadata": {},
   "source": [
    "For this exercise, we only trained on a small subset of the training data, and for a limited number of training epochs. This means that the CNN hasn't yet reached a plateau, and it's not fully trained. You will see better performance by training for more epochs (until the loss values plateau), which you can configure back in [Section 0](#0.configure-training-run) by changing `num_epochs`. You can also train on a larger piece of the training data by configuring `max_n` in the same cell. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fdb25753",
   "metadata": {},
   "source": [
    "## 5. Test the CNN performance\n",
    "\n",
    "Now we use the trained CNN to predict stellar rotation periods from a held-out test set, and compare the predictions to the true values. \n",
    "\n",
    "Since we're only training over a small fraction of the training set in this tutorial, we don't expect the predictions to match the true values. You will see better performance by training on a larger piece (or all) of the training set, or training for more epochs, by configuring `max_n` or `num_epochs` back in [Section 0](#0.configure-training-run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad00a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate CNN to infer rotation periods\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=50)\n",
    "test_loss, preds, trues = evaluate(model, test_loader, verbose=True, return_predictions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26265fdd-5f9e-45e7-b911-076413d854ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the CNN predictions to the true values\n",
    "%matplotlib inline\n",
    "\n",
    "true_periods = trues*test_dataset.pmax\n",
    "pred_periods = preds[:, 0]*test_dataset.pmax\n",
    "pred_sigma = preds[:, 1]*test_dataset.pmax\n",
    "\n",
    "plt.scatter(true_periods, pred_periods, c=pred_sigma)\n",
    "plt.plot([0, 180], [0, 180], \"k\")\n",
    "plt.xlabel(\"True Period (days)\")\n",
    "plt.ylabel(\"Predicted Period (days)\")\n",
    "plt.colorbar(label=\"Predicted Period Uncertainty (days)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf21f86d-f35a-49e1-9175-bba8672a40a7",
   "metadata": {},
   "source": [
    "Most of the data are predicted to have the median period of 90 days, which is the best guess when the CNN doesn't know better. We also predicted the uncertainty in the period, so let's see what that looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee0fc3e-cba3-4a0b-ae00-d7bd0a71327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = preds[:, 1]*test_dataset.pmax\n",
    "frac = sigmas/pred_periods\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(sigmas, bins=20)\n",
    "plt.xlabel(\"Predicted Period Uncertainty (days)\")\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(frac, bins=20)\n",
    "plt.xlabel(\"Fractional Period Uncertainty\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68d6882-9de6-4e3f-a7b0-352a285eb94d",
   "metadata": {},
   "source": [
    "We can start to see a dip forming at sigma/period ~ 0.5 in the second plot (this dip will become more pronounced for longer training runs), so let's use that to filter the results, weeding out \"bad\" predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621c84dd-1a8f-4e55-b831-646ae4cbc9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_filtered = true_periods[frac < 0.5]\n",
    "pred_filtered = pred_periods[frac < 0.5]\n",
    "plt.scatter(true_filtered, pred_filtered)\n",
    "plt.plot([0, 180], [0, 180], \"k\")\n",
    "plt.xlabel(\"True Period (days)\")\n",
    "plt.ylabel(\"Predicted Period (days)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058fba43-4638-4967-a5f7-064d5488afaa",
   "metadata": {},
   "source": [
    "With the \"bad\" predictions filtered out, the predictions look much better, even after only 20 epochs. Remember that the uncertainty is predicted from the quality of the data, so this kind of cut can be applied to predictions on real data as well. \n",
    "\n",
    "While our loss function can serve as a metric of accuracy, we might also be interested in more classical accuracy metrics to measure the performance of the CNN. As an example, let's take a look at the root-mean-squared (RMS) error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9cbdfe-44f3-4dae-9da3-caea46be9258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms_error(true, pred):\n",
    "    return np.sqrt(np.mean((true - pred)**2))\n",
    "\n",
    "print(f\"Unfiltered prediction count: {len(pred_periods)}\\n\"\n",
    "      f\"RMS error: {rms_error(true_periods, pred_periods):.2f} days\\n\\n\"\n",
    "      f\"Filtered prediction count:   {len(pred_filtered)}\\n\"\n",
    "      f\"RMS error: {rms_error(true_filtered, pred_filtered):.2f} days.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508fe865-deac-4253-b18d-5221616e277f",
   "metadata": {},
   "source": [
    "When the predictions are filtered, we retain only a fraction of the test data, but the accuracy improves significantly. Some takeaway notes:\n",
    "\n",
    "- Filtering by predicted uncertainty improves the accuracy. This implies that the predicted uncertainty is a useful estimator of the true credibility of CNN predictions.\n",
    "- While only a fraction of the test set is left after filtering, remember that we trained for only 20 epochs, and on a subset of the training data. Doing a full run will improve both the accuracy of predictions *and* the number of \"good\" predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265ab1b2-fb1a-4293-acc6-ddfc01f5ef14",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Summary goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f52bf38-11f3-4a90-8bc8-a8fdc3c65941",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "References go here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
