{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e8db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/zclaytor/smartsnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71cc70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from smartsnn.model import ConvNet\n",
    "from smartsnn.model import Laplacian_NLL as loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a002abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmax = 180\n",
    "\n",
    "def scale_data(x):\n",
    "    return x/pmax\n",
    "\n",
    "def unscale_data(y):\n",
    "    return y*pmax\n",
    "\n",
    "class WaveletDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path, mode):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mode (string): either train, val, or test\n",
    "        \"\"\"\n",
    "        # need to handle mode; separate train/valid/test data\n",
    "        if mode == \"train\":\n",
    "            filenames = sorted(glob(data_path + \"/001/*fits\"))[0:3000:3] # How to read \"all\" data?\n",
    "        elif mode == \"val\":\n",
    "            filenames = sorted(glob(data_path + \"/001/*fits\"))[1:3001:3]\n",
    "        elif mode == \"test\":\n",
    "            filenames = sorted(glob(data_path + \"/001/*fits\"))[2:3002:3]\n",
    "\n",
    "        periods = []\n",
    "        wavelets = []\n",
    "        \n",
    "        for filename in filenames: # How to read \"all\" data?\n",
    "            f = fits.open(filename)\n",
    "            periods.append(f[0].header[\"PERIOD\"])\n",
    "            wavelets.append(f[2].data)\n",
    "            f.close()\n",
    "        self.wavelets = np.stack(wavelets)\n",
    "        self.periods = np.array(periods)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.periods)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        X = self.wavelets[idx].astype('float32') \n",
    "        X = torch.tensor(X/255)\n",
    "        X = torch.unsqueeze(X, 0)\n",
    "        label = torch.tensor(self.periods[idx, np.newaxis])\n",
    "        return X, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034d9728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, num_epochs=100, early_stopping_patience=10, device=torch.device(\"cpu\")):\n",
    "    '''Train the neural network for all desired epochs.\n",
    "    '''\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "    # Set learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(optimizer, factor=0.7, patience=3)\n",
    "\n",
    "    # Training loop\n",
    "    train_p_loss = []\n",
    "    val_p_loss = []\n",
    "    min_loss = 100\n",
    "    early_stopping_count = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        p_loss = train_epoch(model, train_loader, optimizer, epoch, device=device)\n",
    "        train_p_loss.append(p_loss)\n",
    "        #p_loss = test(model, device, train_loader, epoch, mode=\"train\", make_plot=True, verbose=False)\n",
    "        p_loss = test(model, val_loader, epoch, mode=\"val\")\n",
    "        val_p_loss.append(p_loss)\n",
    "        total_loss = p_loss\n",
    "        scheduler.step(total_loss)    # learning rate scheduler\n",
    "\n",
    "        if total_loss < min_loss:\n",
    "            min_loss = total_loss\n",
    "            early_stopping_count = 0\n",
    "            best_epoch = epoch\n",
    "            best_weights = deepcopy(model.state_dict())\n",
    "        else:\n",
    "            early_stopping_count += 1\n",
    "            print(f'Early Stopping Count: {early_stopping_count}')\n",
    "            if early_stopping_count == early_stopping_patience:\n",
    "                print(f\"Early Stopping. Best Epoch: {best_epoch} with loss {min_loss:.4f}.\")\n",
    "                with open(\"best_epoch.txt\", \"w\") as f:\n",
    "                    print(best_epoch, file=f)\n",
    "                break    \n",
    "\n",
    "    torch.save(best_weights, f\"model.pt\")\n",
    "    return best_weights, train_p_loss, val_p_loss\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, epoch, device=torch.device(\"cpu\")):\n",
    "    '''\n",
    "    This is your training function. When you call this function, the model is\n",
    "    trained for 1 epoch.\n",
    "    '''\n",
    "    model.train() # Set the model to training mode\n",
    "    period_losses = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device, dtype=torch.float), target.to(device, dtype=torch.float)\n",
    "        optimizer.zero_grad()               # Clear the gradient\n",
    "        output = model(data)                # Make predictions\n",
    "        loss = loss_function(output, target)\n",
    "        loss.backward()                     # Gradient computation        \n",
    "        optimizer.step()                    # Perform a single optimization step\n",
    "        period_losses.append(loss.item())\n",
    "\n",
    "        if (batch_idx*len(data)) % 10000 == 0:\n",
    "            print('Epoch: {} [{}/{} ({:3.0f}%)] Training Loss: {:9.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), period_losses[-1]), end=\" \")\n",
    "    return np.mean(period_losses)\n",
    "\n",
    "def test(model, test_loader, epoch=None, mode=None, verbose=True, device=torch.device(\"cpu\")):\n",
    "    model.eval()    # Set the model to inference mode\n",
    "    test_p_loss = 0\n",
    "    targets = []\n",
    "    preds = []\n",
    "    with torch.no_grad():   # For the inference step, gradient is not computed\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device, dtype=torch.float), target.to(device, dtype=torch.float)\n",
    "            output = model(data)\n",
    "            targets.extend(target.cpu().numpy())\n",
    "            preds.extend(output.cpu().numpy())\n",
    "            test_p_loss += loss_function(output, target, reduction='sum').item()\n",
    "\n",
    "    test_p_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Test loss: {test_p_loss:.4f}')\n",
    "    return test_p_loss\n",
    "\n",
    "def predict(model, test_loader, verbose=True, device=torch.device(\"cpu\")):\n",
    "    model.eval()    # Set the model to inference mode\n",
    "    preds = []\n",
    "    labels = []\n",
    "    predicted = 0\n",
    "    test_p_loss = 0\n",
    "    with torch.no_grad():   # For the inference step, gradient is not computed\n",
    "        for data, target in test_loader:\n",
    "            labels.extend(np.array(target))\n",
    "            data, target = data.to(device, dtype=torch.float), target.to(device, dtype=torch.float)\n",
    "            output = model(data)\n",
    "            test_p_loss += loss_function(output, target, reduction='sum').item()\n",
    "\n",
    "            preds.extend(output.cpu().numpy())\n",
    "            predicted += len(target)\n",
    "            #print(\"Progress: {}/{}\".format(predicted, len(test_loader.dataset)))\n",
    "\n",
    "    test_p_loss /= len(test_loader.dataset)\n",
    "    if verbose:\n",
    "        print(f'Test loss: {test_p_loss:.4f}')\n",
    "\n",
    "\n",
    "        return np.squeeze(preds), np.squeeze(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754d5099",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_dataset = WaveletDataset(data_path=\"data\", mode=\"train\")\n",
    "valid_dataset = WaveletDataset(data_path=\"data\", mode=\"val\")\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=50)\n",
    "val_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=50)\n",
    "\n",
    "model = ConvNet()\n",
    "\n",
    "weights, train_p_loss, val_p_loss = train(model, train_loader, val_loader,\n",
    "    early_stopping_patience=10, num_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad00a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = WaveletDataset(data_path=\"data\", mode=\"test\")\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=50)\n",
    "\n",
    "preds, trues = predict(model, test_loader, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d164df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.scatter(trues, preds[:,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
