{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d30c40a",
   "metadata": {},
   "source": [
    "# Regressing TESS rotation periods with CNNs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb9b70ac",
   "metadata": {},
   "source": [
    "TESS systematics make it difficult to measure long (> 13 day) rotation periods from TESS light curves using conventional frequency analysis. In this tutorial we will use a Convolutional Neural Network (CNN) to regress periods from TESS light curve transforms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1b3c64b",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The goal of this notebook is to use [SMARTS](https://archive.stsci.edu/hlsp/smarts) data with the open-source CNN code [`smartsnn`](https://github.com/zclaytor/smartsnn) to regress TESS rotation periods. We will\n",
    "1. Read in a subset of the training data,\n",
    "2. Define training, validation, and evaluation functions,\n",
    "3. Train the CNN on SMARTS data,\n",
    "4. Predict rotation periods using SMARTS test data, and\n",
    "5. Compare the predicted periods to the true simulation periods.\n",
    "\n",
    "The training examples are 2-dimensional arrays of wavelet transforms of TESS light curves. The wavelet transform concentrates the periodicity of the light curve, making it easier for a CNN to regress the period. CNNs take advantage of the image-like nature of a wavelet transform in the same way that CNNs are useful for image recognition and computer vision."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "591c9b26",
   "metadata": {},
   "source": [
    "## Installs and Imports\n",
    "This notebook uses the following packages:\n",
    "- `smartsnn` for building and using the CNN\n",
    "- `glob` for generating lists of training files\n",
    "- `copy` for saving training weights\n",
    "- `numpy` for array operations\n",
    "- `matplotlib` for plotting\n",
    "- `astropy` for reading FITS files\n",
    "- `torch` for tensor and CNN operations\n",
    "\n",
    "`smartsnn` is on [GitHub](https://github.com/zclaytor/smartsnn); we install it first. The other packages can be installed using `pip` or `conda`.\n",
    "\n",
    "We will also need to download and uncompress the training data from MAST. Let's do that first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8513c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data\n",
    "!curl -o hlsp_smarts_tess_ffi_all_tess_v1.0_cat.tar.gz https://archive.stsci.edu/hlsps/smarts/hlsp_smarts_tess_ffi_all_tess_v1.0_cat.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae753d0-0d4d-49b2-8fe1-eaea47f79475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training data\n",
    "!tar -xvzf hlsp_smarts_tess_ffi_all_tess_v1.0_cat.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e8db64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install needed packages\n",
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71cc70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob # for generating lists of input files\n",
    "from copy import deepcopy # for saving CNN weights\n",
    "\n",
    "import numpy as np # array operations\n",
    "from astropy.io import fits # FITS file operations\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "\n",
    "# For tensor and CNN operations\n",
    "import torch \n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# For defining and training the CNN\n",
    "from smartsnn.model import ConvNet\n",
    "from smartsnn.model import Laplacian_NLL as loss_function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "766eb96b",
   "metadata": {},
   "source": [
    "## 1. Define training data loader\n",
    "We use Pytorch `Dataset` (subclassed here) and `DataLoader` (used below) to load the wavelet data. Pytorch `Dataset` and `DataLoader` can be accessed in chunks, which makes training more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a002abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveletDataset(Dataset):\n",
    "    \"\"\"\n",
    "    WaveletDataset to read in the training data.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    `wavelets`: array containing the stack of wavelet transforms.\n",
    "\n",
    "    `periods`: array of rotation period corresponding to each wavelet transform.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, periods, wavelets, mode, random_seed=42, max_n=10000, split=(8, 1, 1), normalize=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        `periods` (numpy.ndarray): the array of rotation periods, or labels.\n",
    "\n",
    "        `wavelets` (numpy.ndarray): the array of wavelet power spectra, or features.\n",
    "\n",
    "        `mode` (str): must be one of \"train\", \"val\", or \"test\". Loads different data\n",
    "            depending on the specified mode.\n",
    "\n",
    "        `random_seed` (int, optional): seed for random number generator for \n",
    "            reproducibility.\n",
    "\n",
    "        `max_n` (int, optional): the number of training examples to use.\n",
    "\n",
    "        `split` (list-like): the split fractions for train/validation/test partitions\n",
    "\n",
    "        `normalize` (bool): whether to divide the periods and wavelets by their maximum values.\n",
    "        \"\"\"\n",
    "\n",
    "        # create shuffled index and shuffle arrays\n",
    "        np.random.seed(random_seed)\n",
    "        idx = np.random.choice(np.arange(len(periods), dtype=int), size=max_n, replace=False)\n",
    "        p = periods[idx]\n",
    "        w = wavelets[idx]\n",
    "\n",
    "        if normalize:\n",
    "            pmax = periods.max().item()\n",
    "            wmax = wavelets.max().item()\n",
    "            p = (p/pmax).astype(np.float32)\n",
    "            w = (w/wmax).astype(np.float32)\n",
    "        else:\n",
    "            pmax = np.nan\n",
    "            wmax = np.nan\n",
    "            p = p.astype(np.float32)\n",
    "            w = w.astype(np.uint8)\n",
    "        self.pmax = pmax\n",
    "        self.wmax = wmax\n",
    "\n",
    "        # determine how many examples to use for each partition\n",
    "        n_train, n_val, n_test = (max_n * np.array(split)/np.sum(split)).astype(int).cumsum()\n",
    "\n",
    "        if mode == \"train\":\n",
    "            p = p[:n_train]\n",
    "            w = w[:n_train]\n",
    "        elif mode == \"val\":\n",
    "            p = p[n_train:n_val]\n",
    "            w = w[n_train:n_val]\n",
    "        elif mode == \"test\":\n",
    "            p = p[n_val:]\n",
    "            w = w[n_val:]\n",
    "        else:\n",
    "            raise ValueError(\"`mode` must be one of 'train', 'val', or 'test'.\")\n",
    "                 \n",
    "        # Assign periods and wavelets to class attributes\n",
    "        self.wavelets = w\n",
    "        self.periods = p\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of training examples in the Dataset.\n",
    "        \"\"\"\n",
    "        return len(self.periods)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        The data accessor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        `idx` (list-like): the list of indices to be accessed\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        `X` (tensor): substack of wavelet transforms\n",
    "\n",
    "        `label` (tensor): sub-array of rotation periods\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        X = torch.tensor(self.wavelets[idx], dtype=torch.float32).unsqueeze(0)\n",
    "        label = torch.tensor(self.periods[idx, np.newaxis])\n",
    "        return X, label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48006ccb",
   "metadata": {},
   "source": [
    "## 2. Define training, validation, and evaluation functions\n",
    "We train the CNN using the Adam optimizer and log-Laplacian as our loss function. The Adam optimizer uses adaptive learning rates to train the network, and the log-Laplacian loss lets us predict the uncertainty in the estimated rotation period.\n",
    "\n",
    "A separate, held-out validation set is used to determine when to stop training. The training loss will decrease indefinitely, but the validation loss will stop decreasing when the CNN begins to overfit, signalling that it's reached a local maximum in its ability to generalize to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034d9728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, num_epochs=100, early_stopping_patience=10, device=torch.device(\"cpu\")):\n",
    "    \"\"\"Train the neural network for all desired epochs.\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "    # Set learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(optimizer, factor=0.7, patience=3)\n",
    "\n",
    "    # Set up training loop\n",
    "    train_p_loss = []\n",
    "    val_p_loss = []\n",
    "    min_loss = 100\n",
    "    early_stopping_count = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Compute a single epoch of training\n",
    "        p_loss = train_epoch(model, train_loader, optimizer, epoch, device=device)\n",
    "        train_p_loss.append(p_loss)\n",
    "        p_loss = test(model, val_loader, epoch, mode=\"val\")\n",
    "        val_p_loss.append(p_loss)\n",
    "        total_loss = p_loss\n",
    "        scheduler.step(total_loss)    # step learning rate scheduler\n",
    "\n",
    "        # if new fit is better than the previous best fit, update best fit weights\n",
    "        if total_loss < min_loss:\n",
    "            min_loss = total_loss\n",
    "            early_stopping_count = 0\n",
    "            best_epoch = epoch\n",
    "            best_weights = deepcopy(model.state_dict())\n",
    "        # otherwise, if loss is not getting better, count down to stopping criterion\n",
    "        else:\n",
    "            early_stopping_count += 1\n",
    "            print(f'Early Stopping Count: {early_stopping_count}')\n",
    "            if early_stopping_count == early_stopping_patience:\n",
    "                print(f\"Early Stopping. Best Epoch: {best_epoch} with loss {min_loss:.4f}.\")\n",
    "                with open(\"best_epoch.txt\", \"w\") as f:\n",
    "                    print(best_epoch, file=f)\n",
    "                break    \n",
    "\n",
    "    # save and return the best fit weights\n",
    "    torch.save(best_weights, f\"model.pt\")\n",
    "    return best_weights, train_p_loss, val_p_loss\n",
    "\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, epoch, device=torch.device(\"cpu\")):\n",
    "    \"\"\"Train the network for a single epoch.\n",
    "    \"\"\"\n",
    "    model.train() # Set the model to training mode\n",
    "    period_losses = []\n",
    "    # iterate over data batches\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device, dtype=torch.float), target.to(device, dtype=torch.float)\n",
    "        optimizer.zero_grad()               # Clear the gradient\n",
    "        output = model(data)                # Make predictions\n",
    "        loss = loss_function(output, target)\n",
    "        loss.backward()                     # Gradient computation        \n",
    "        optimizer.step()                    # Perform a single optimization step\n",
    "        period_losses.append(loss.item())\n",
    "\n",
    "        # print progress every few batches\n",
    "        if (batch_idx*len(data)) % 500 == 0:\n",
    "            print('Epoch: {:3d} [{:5d}/{:5d} ({:3.0f}%)] Training Loss: {:9.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), period_losses[-1]))\n",
    "            \n",
    "    # return the mean loss value\n",
    "    return np.mean(period_losses)\n",
    "\n",
    "\n",
    "def test(model, test_loader, epoch=None, mode=None, verbose=True, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Evaluate network on validation or test data and compute loss.\n",
    "    This is like \"train\" except the weights are not updated.\n",
    "    \"\"\"\n",
    "    model.eval()    # Set the model to inference mode\n",
    "    test_p_loss = 0\n",
    "    targets = []\n",
    "    preds = []\n",
    "    with torch.no_grad():   # For the inference step, gradient is not computed\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device, dtype=torch.float), target.to(device, dtype=torch.float)\n",
    "            output = model(data)\n",
    "            targets.extend(target.cpu().numpy())\n",
    "            preds.extend(output.cpu().numpy())\n",
    "            test_p_loss += loss_function(output, target, reduction='sum').item()\n",
    "\n",
    "    test_p_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Test loss: {test_p_loss:.4f}')\n",
    "    return test_p_loss\n",
    "\n",
    "\n",
    "def predict(model, test_loader, verbose=True, device=torch.device(\"cpu\")):\n",
    "    \"\"\"Evaluate the neural network and return predictions.\n",
    "    \"\"\"\n",
    "    model.eval()    # Set the model to inference mode\n",
    "    preds = []\n",
    "    labels = []\n",
    "    predicted = 0\n",
    "    test_p_loss = 0\n",
    "    with torch.no_grad():   # For the inference step, gradient is not computed\n",
    "        for data, target in test_loader:\n",
    "            labels.extend(np.array(target))\n",
    "            data, target = data.to(device, dtype=torch.float), target.to(device, dtype=torch.float)\n",
    "            output = model(data)\n",
    "            test_p_loss += loss_function(output, target, reduction='sum').item()\n",
    "\n",
    "            preds.extend(output.cpu().numpy())\n",
    "            predicted += len(target)\n",
    "\n",
    "    test_p_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Test loss: {test_p_loss:.4f}')\n",
    "    return np.squeeze(preds), np.squeeze(labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a59eb327",
   "metadata": {},
   "source": [
    "## 3. Train the CNN on SMARTS data\n",
    "\n",
    "Ideally, we want to run on the full training set (`max_n = 1_000_000`) for long enough that the loss plateaus (usually `num_epochs = 500`). But this for this simple demo, we'll use a subset of training data and a shorter training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69e4964-0259-4b9f-a016-7ab818ebc555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training run\n",
    "max_n = 10_000 # max 1_000_000\n",
    "num_epochs = 10 # 100 is pretty good, typically ~300 to reach a plateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754d5099",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read SMARTS data into data loaders\n",
    "filename = \"hlsp_smarts_tess_ffi_all_tess_v1.0_sim.fits\"\n",
    "print(f\"Reading data from {filename}...\", end=\"\")\n",
    "\n",
    "with fits.open(filename, memmap=True) as f:\n",
    "    p = f[1].data[\"Period\"]\n",
    "    w = f[2].data\n",
    "    train_dataset = WaveletDataset(p, w, mode=\"train\", max_n=max_n)\n",
    "    valid_dataset = WaveletDataset(p, w, mode=\"val\", max_n=max_n)\n",
    "    test_dataset = WaveletDataset(p, w, mode=\"test\", max_n=max_n)\n",
    "\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Storing training data into DataLoaders...\", end=\"\")\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=50)\n",
    "val_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=50)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c27331-b0e0-4286-8de1-d98693ea357b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Begin training:\")\n",
    "\n",
    "# Initialize CNN\n",
    "model = ConvNet()\n",
    "\n",
    "# Train CNN\n",
    "weights, train_p_loss, val_p_loss = train(model, train_loader, val_loader,\n",
    "    early_stopping_patience=10, num_epochs=num_epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fdb25753",
   "metadata": {},
   "source": [
    "## 4. Predict rotation periods from a held-out test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad00a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate CNN to infer rotation periods\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=50)\n",
    "preds, trues = predict(model, test_loader, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2094d10a",
   "metadata": {},
   "source": [
    "## 5. Compare the predictions to the true values\n",
    "and note that since we're only training over a small fraction of the training set in this tutorial, we don't necessarily expect the predictions to match the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d164df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "true_periods = trues*test_dataset.pmax\n",
    "pred_periods = preds[:, 0]*test_dataset.pmax\n",
    "pred_sigma = preds[:, 1]*test_dataset.pmax\n",
    "\n",
    "plt.scatter(true_periods, pred_periods, c=pred_sigma)\n",
    "plt.plot([0, 180], [0, 180], \"k\")\n",
    "plt.xlabel(\"True Period (days)\")\n",
    "plt.ylabel(\"Predicted Period (days)\")\n",
    "plt.colorbar(label=\"Predicted Period Uncertainty (days)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf21f86d-f35a-49e1-9175-bba8672a40a7",
   "metadata": {},
   "source": [
    "Most of the data are predicted to have the median period of 90 days, which is the best guess when the CNN doesn't know better. We also predicted the uncertainty in the period, so let's see what that looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee0fc3e-cba3-4a0b-ae00-d7bd0a71327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = preds[:, 1]*test_dataset.pmax\n",
    "frac = sigmas/pred_periods\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(sigmas, bins=20)\n",
    "plt.xlabel(\"Predicted Period Uncertainty (days)\")\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(frac, bins=20)\n",
    "plt.xlabel(\"Fractional Period Uncertainty\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68d6882-9de6-4e3f-a7b0-352a285eb94d",
   "metadata": {},
   "source": [
    "There's a dip at sigma/period ~ 0.5, so let's use that to filter the results, weeding out \"bad\" predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621c84dd-1a8f-4e55-b831-646ae4cbc9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_filtered = true_periods[frac < 0.5]\n",
    "pred_filtered = pred_periods[frac < 0.5]\n",
    "plt.scatter(true_filtered, pred_filtered)\n",
    "plt.plot([0, 180], [0, 180], \"k\")\n",
    "plt.xlabel(\"True Period (days)\")\n",
    "plt.ylabel(\"Predicted Period (days)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058fba43-4638-4967-a5f7-064d5488afaa",
   "metadata": {},
   "source": [
    "With the \"bad\" predictions filtered out, the predictions look much better! Remember that the uncertainty is predicted from the quality of the data, so this kind of cut can be applied to predictions on real data as well. \n",
    "\n",
    "Finally, let's define an accuracy metric to measure the performance. For this example, we'll use the root-mean-squared (RMS) error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9cbdfe-44f3-4dae-9da3-caea46be9258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms_error(true, pred):\n",
    "    return np.sqrt(np.mean((true - pred)**2))\n",
    "\n",
    "print(f\"Unfiltered prediction count: {len(pred_periods)}\\n\"\n",
    "      f\"RMS error: {rms_error(true_periods, pred_periods):.2f} days\\n\\n\"\n",
    "      f\"Filtered prediction count:   {len(pred_filtered)}\\n\"\n",
    "      f\"RMS error: {rms_error(true_filtered, pred_filtered):.2f} days.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508fe865-deac-4253-b18d-5221616e277f",
   "metadata": {},
   "source": [
    "When the predictions are filtered, we retain only about 20% of the sample, but the accuracy improves significantly. Some takeaway notes:\n",
    "\n",
    "- Filtering by predicted uncertainty improves the accuracy. This implies that the predicted uncertainty is a useful estimator of the true credibility of CNN predictions.\n",
    "- While only 20% of the sample is left after filtering, remember that we only trained for 100 epochs, and on a subset of the training data. Doing a full run will improve both the accuracy of predictions *and* the number of \"good\" predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
